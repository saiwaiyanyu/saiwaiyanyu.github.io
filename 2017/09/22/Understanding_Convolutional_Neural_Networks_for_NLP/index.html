<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>卷积神经网络在自然语言处理中的应用 | 塞外烟雨</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">卷积神经网络在自然语言处理中的应用</h1><a id="logo" href="/.">塞外烟雨</a><p class="description">假装会NLP和KG的攻城狮</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">卷积神经网络在自然语言处理中的应用</h1><div class="post-meta"><a href="/2017/09/22/Understanding_Convolutional_Neural_Networks_for_NLP/#comments" class="comment-count"></a><p><span class="date">Sep 22, 2017</span><span><a href="/categories/自然语言处理/" class="category">自然语言处理</a><a href="/categories/自然语言处理/机器学习/" class="category">机器学习</a><a href="/categories/自然语言处理/机器学习/翻译/" class="category">翻译</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>博客原文 <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolutional Neural Networks for NLP</a></p>
<p>当听到卷积神经网络 (<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" rel="external">Convolutional Neural Network ,CNNs</a>) 的时候， 我们常会联想到计算机视觉。CNNs 是图像分类领域取得重大突破的一个重要因素。如今，从 Facebook 图像自动标签应用到自动驾驶领域，CNNs 俨然成了计算机视觉系统的关键技术。</p>
<p>近期，我们已经着手尝试将 CNNs 应用到自然语言处理（Natural Language Processing，NLP），并且已经取得一些初步进展。我将在本文中归纳，什么是CNNs？以及如何将其应用到NLP？从计算机视觉的用例出发，更容易理解 CNNs背后的逻辑，因此，我们先从这里开始，然后逐渐过渡到NLP。</p>
<p>最简洁、直观的理解卷积（Convolution），就是把它想象成应用到矩阵的滑动窗口函数。这种表述比较口语化了，我们用一个动画来表示就更直观了：</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Convolution_schematic.gif" alt=""><br></div>

<p> *3x3的滤波器做卷积运算。图片来源： <a href="http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution" target="_blank" rel="external">http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution</a> </p>
<p>左侧的矩阵可以想象成是一个黑白图像的表征， 每一个元素都表示一个像素点，0-表示黑点，1-表示白点（灰度的图像的像素值范围一般是0-255）。滑动窗口又称作核、滤波器或是特征检测器。这里我们使用3x3的滤波器，将滤波器和矩阵对应的部分逐个元素相乘，并求和。通过平移窗口，使滤波器滑过矩阵的所有像素，可以对整个图像做卷积运算。</p>
<p>也许你会疑问，通过上述操作，我们能够得到什么呢？以下是几个直观的例子：</p>
<p>用图像像素及其邻近的像素平均值，达到模糊图像的效果。 </p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/generic-taj-convmatrix-blur.jpg" alt=""><br></div>

<p>用邻近点像素值与自身的差值替换其原值，实现边缘检测的效果 </p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/generic-taj-convmatrix-edge-detect.jpg" alt=""><br></div>

<p><a href="https://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">GIMP手册</a>里还其他的例子。如果想要深入了解卷积运算的原理，我推荐阅读<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">Chris Olah的博客</a>。</p>
<h2 id="什么是卷积神经网络"><a href="#什么是卷积神经网络" class="headerlink" title="什么是卷积神经网络"></a>什么是卷积神经网络</h2><p>现在你应该知道什么是卷积了。但是什么是卷积神经网络（CNNs）又是什么呢？CNNs其本质是多层卷积的叠加运算，并结合类似ReLU或tanh等非线性激活函数的结果。</p>
<p>在传统的前馈神经网络（feedforward neural network）中，我们把每一个输入神经元和下一层的输入神经元相连。这种处理方式也叫做全连接层（fully connected layer），或者仿射层（affine layer）。 在CNNs处理时，我们并不采用这样的方式，而是采用输入层的卷积结果来计算输出。这等效于于是局部连接（local connections），每个局部的输入区域与输出的一个神经元相连接。对每一层应使用同的滤波器，滤波器通常是如上图所示的成百上千个，然后汇总它们的结果。这里也涉及到池化层（降采样）（pooling (subsampling) layers），我会在后文做解释。在训练阶段，CNN基于你想完成的任务自动学习滤波器的权重值。举个例子，在图像分类问题中，第一层CNN模型或许能学会从原始像素点检测到一些边缘线条，然后根据边缘线条在第二层检测出一些简单的形状，然后基于这些形状检测出更高级的特征，比如脸部轮廓等。最后一层是利用这些高级特征的一个分类器。</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-07-at-7.26.20-AM-1024x279.png" alt=""><br></div>

<p>这种计算方式时，有两点值得注意: 位置不变性（Location Invariance）和组合性（Compositionality）。例如你想识别一张图片里面是否包含大象，因为滤波器是在整个图片上滑动，因此，并不关心大象在图片的具体位置。事实上，池化处理也有助于平移、旋转和缩放的不变性，它对克服缩放因素的效果尤其好。</p>
<p>第二个关键点是（局部）组合性（(local) compositionality）。每个滤波器对一小块局部区域的低级特征组合形成更高级的特征表示。这也是CNNs对计算机视觉作用巨大的原因。我们可以很直观地理解，线条由像素点构成，基本形状又由线条构成，更复杂的物体又源自基本的形状。</p>
<h2 id="如何将CNN应用到NLP？"><a href="#如何将CNN应用到NLP？" class="headerlink" title="如何将CNN应用到NLP？"></a>如何将CNN应用到NLP？</h2><p>不同于图像的像素，大部分NLP的处理的任务是通过矩阵来表示的句子或者文档。<br>矩阵的每一行表示一个词元素（token），例如是一个词，或者是一个字符。<br>也即是说，每行都是表示一个词的向量。通常，这些向量是  word embeddings (low-dimensional representations) 例如 word2vec 和 GloVe，也可以是one-hot 向量，即是词在词表（vocabulary）中的索引（index）位置。<br>对于一个19个词的句子，用100-维的enbedding表示，能够得到10×100的矩阵，这就是我们得到的“图像”。</p>
<p>在计算机视觉处理时，滤波器会滑过图片的局部区域（ local patches），但是在NLP时，一般是利用滤波器滑过矩阵的所有行（词）。因此，滤波器的“宽度”一般是输入矩阵的宽度，高度或者区域大小，可以调节，但是一次滑动2-5个词是比较常见的做法。综上，CNNs用来处理NLP的过程可以看成这样：</p>
<div align="center"><br> <img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-06-at-12.05.40-PM.png" width="600" height="500"><br></div> 

<p>应用于文本句子分类的 CNNs 结构示意图。我们描述三种尺寸的滤波器: 2, 3 和 4, 美中尺寸都有 2 种滤波器。每个滤波器都对文本句子矩阵进行卷积运算操作，得到特征词典。然后在每个特征字典上做最大池化处理，也即是记录下每个特征字典的最大。这样，从所有的6个特征字典生成得到一个单变量特征向量（univariate feature vector），并且这6个特征拼接成一个特征向量，传入神经网络的倒数第二层 （penultimate layer）。最终softmax层接收这个特征向量作为输入，并用作句子分类。我们假设这是一个二分类问题，最终输出就是两个可能的状态（state） 。<br>图片来源: Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification. </p>
<p>到此，是否已有计算机视觉的直观感觉？局部不变性（Location Invariance ）和局部组合性（local Compositionality）对图像计算很直观，但是对于NLP且不失如此。<br>也许你会关注到单词在句子中出现的位置，像素和相邻的点可能是相关联的，（都是同一物体的一部分），但是对于单词却不尽然。在许多种语言里, 短语会被多个词语分割。组合型也不是那么显然， 显然，单词会以某种方式组合在一起，例如形容词用做修饰名词，但是，想了解这些高级特征的真正的“含义”，却不如计算机视觉那么直观了。</p>
<p>综上看来，好像CNNs并不能很好的处理NLP任务。 <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">递归神经网络（Recurrent Neural Networks，RNN）</a>看起来更直观。它们模仿人类处理语言的逻辑方式（至少是我们认为我们处理的方式） ：从左往右阅读。 幸运的是, 这并不意味CNNs就无效了。<a href="https://en.wikipedia.org/wiki/All_models_are_wrong" target="_blank" rel="external">如果不能被正确的使用，那么所有的模型都是错的</a>。<br>事实证明，CNNs用来处理NLP表现非常好。<br><a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank" rel="external">词袋模型（Bag of Words model ）</a>是一个基于错误假设的简化模型，但是这并不妨碍它成为多年处理NLP的标准方式，并且表现良好。</p>
<p>CNNs的一个主要特点就是训练速度快，非常快。 卷积运算是计算机图形的核心部分，并在GPUs的硬件层实现。相比于<a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="external">n-gram</a>，CNNs 在表征方式上也十分高效。在一个超大的词典上，计算任何超过3-grams的都会非常大的开销。即使是google也没法在超过5-ngrams上进行处理。在不需要表征全量词典的条下，卷积滤波器（Convolutional Filters）会自动学习较好的表征方式。选择尺寸超过5的滤波器也是完全合理的。 个人认为，在底层学到的滤波器能够获取到n-grams想尽的特征，尽管这些特征是以更加紧凑的方式表征。</p>
<h2 id="CNN模型的超参数"><a href="#CNN模型的超参数" class="headerlink" title="CNN模型的超参数"></a>CNN模型的超参数</h2><p>在谈到如何用CNNs来处理NLP任务前，可以先看一下在构建CNN模型时需要面临的几个选择。希望这些内容能够帮助你更好的理解该领域相关的文献。</p>
<h3 id="窄卷积和宽卷积"><a href="#窄卷积和宽卷积" class="headerlink" title="窄卷积和宽卷积"></a>窄卷积和宽卷积</h3><p>在前文解释什么是卷积时，我忽略了使用滤波器时的一个小细节。<br>在矩阵中间使用 3×3 的滤波器没有问题，但是在矩阵边缘该如何处理呢？<br>矩阵的第一个元素，在其左面和上面没有临近元素，我们该如何应用滤波器呢？<br>可以采用补零法 （zero-padding），所有超过矩阵的外部元素都取值0。通过这样方式，就可以对输入的矩阵的任何元素都能够应用滤波器了，并且得到一个和输入矩阵同样大小或者更大的结果矩阵。  补零法又叫做宽卷积，不使用补零则是窄卷积。 一个1D的例子如下:</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-05-at-9.47.41-AM.png" alt=""><br></div>

<p>窄卷积 vs. 宽卷积。滤波器大小 5，输入大小 7。来源: A Convolutional Neural Network for Modelling Sentences (2014)</p>
<p>可以看出，当滤波器的长度和输入比较接近时，宽卷积是很有用，甚至说是很有必要的。以上，窄卷积的输出大小是 (7-5) + 1=3，而宽卷积的输出则是 (7+2*4 - 5) + 1 =11。一般地，输出形式可以表示为：</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/latex.png" alt=""><br></div>

<h3 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h3><p>卷积运算的另一个超参数是步长，即滤波器每滑动一次的距离。上面示例中的步长都是1，且相邻的滤波器有重叠。步长越大，则用到的滤波器越少，输出的值也越少。下图来自斯坦福的cs231课程网页，分别是步长为1和2的情况：</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-05-at-10.18.08-AM.png" alt=""><br></div> 

<p>卷积步长。左侧：步长为1，右侧：步长为2。来源： <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a> </p>
<p>在文献中我们常常见到的步长是1，但选择更大的步长会让模型更接近于<a href="https://en.wikipedia.org/wiki/Recursive_neural_network" target="_blank" rel="external">递归神经网络</a>，其结构就像是一棵树。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层是卷积神经网的一个重要概念，一般应用到卷积层之后。 池化层是对输入的降采样（subsample）。池化处理的一般做法是对每个滤波器的输出结果进行最大化。<br>并不需要对全量矩阵进行池化，对其一个窗口进行池化即可。例如，对2x2的窗口进行最大池化（在NLP时，一般是对全量输出进行池化，对每一个滤波器结果得到唯一的值）：</p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-05-at-2.18.38-PM.png" alt=""><br></div> 

<p>CNN 的最大池化处理. Source: <a href="http://cs231n.github.io/convolutional-networks/#pool" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/#pool</a> </p>
<p>为什么要池化处理呢？有多种原因。池化的的一个特点就是输出一个固定大小的矩阵，这是分类问题所需要的。例如，如果有1,000个滤波器，并且对每个都进行最大化处理，那么你会得到一个1000-维德输出，无论给定过的滤波器大小是什么，或者你输入大小。这让你能够可变长度的句子，可变大小的滤波器，但是最终都能得到一样维度的输出，进而传入分类器。</p>
<p>池化处理也能够对输出进行降维，并（理想情况）尽可能的保留有用信息。<br>可以想象成，每一个滤波器都探测一种特殊的特征，例如探测句子是否包含一个负面的特征“not amazing”。如果类似短语在句子的某个位置出现，那么滤波器的在该区域的将会生成一个较大的值，在其他区域则是较小的值。通过最大处操作，能够保留这些特征是否出现在句子的信息，但是会丢失出现的具体位置的信息。但是具体出现具体位置真的有用吗？是的，这就是类似一组n-gram所做的工作。<br>尽管丢了关于位置信息的全局信息（句子的大致位置），但是滤波器保留了局部的信息，例如“not amazing” 和“amazing not”的含义就相差甚远。</p>
<p>在图像识别中，池化还能够具有平移和旋转不变性。当你在一个区域进行池化，即使对徒刑进行了旋转／平移操作，所得到的结果基本一样，因为最大化处理得到的结果总是一样的。</p>
<h3 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h3><p>最后一个需要了解的概念是通道。通道是输入数据的不同“视角”。例如，图像识别时一般会有RGB（红，绿，蓝）通道。当赋予不同或相同的权值时，可以在通道上做卷积运算。在NLP处理时，也可以想象有不同的通道：对于不同的word embeddings (<a href="https://code.google.com/archive/p/word2vec" target="_blank" rel="external">word2vec</a> 和<a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="external"> GloVe</a>  )，你可以有一个特殊的通道 , 或者你可得到一个通道，对句子通过不同的语言表示，或者短语不同的表达。</p>
<h2 id="CNN在自然语言处理中应用"><a href="#CNN在自然语言处理中应用" class="headerlink" title="CNN在自然语言处理中应用"></a>CNN在自然语言处理中应用</h2><p>现在我们看看CNNs如何应用到自然语言处理。我会总结一下该方面的研究成果，我希望能够概括最近的比较流行的研究成果，尽管会遗漏一些有趣的应用。 </p>
<p>对于CNNs最自然的一个应用就是分类任务。例如情感分析，垃圾邮件识别，主题归类，卷积和池化运算会丢失一些局部的词语的顺序信息，因此对于句子序列标记任务或者实体抽取任务对于CNNs来说难度较大，（当然也不是不可能，你可以在输入时增加位置特征）。<br>文献【1】对于评估CNNs主要是在多分类任务上，特别是语义分析和主体分类任务。CNNs在数据集分类上表现不错，有些还刷新了记录。但是出乎意料的是，本文采用的神经网络很简单，但是却取得较好成果。输入成是将句子组合成的word2vec的词向量，接着是由若干个滤波器组成的卷积层，然后是最大池化层，最后是softmax分类器。该论文也尝试了两种不同形式的通道，分别是静态和动态词向量，其中一个通道在训练时动态调整而另一个不变。文献[2]中提到了一个类似的结构，但更复杂一些。文献[6]在网络中又额外添加了一个层，用于语义聚类。 </p>
<div align="center"><br><img src="/img/Understanding_Convolutional_Neural_Networks_for_NLP/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png" alt=""><br></div>

<p>Kim, Y. (2014). 卷积神经网络用来语句分类</p>
<p>文献[4]从原始数据训练CNN模型，不需要预训练得到word2vec或GloVe等词向量表征。它直接对one-hot向量进行卷积运算。作者对输入数据采用了节省空间的类似词袋表征方式，以减少网络需要学习的参数个数。在文献[5]中作者用了CNN学习得到的非监督式“region embedding”来扩展模型，预测文字区域的上下文内容。这些论文中提到的方法对处理长文本（比如影评）非常有效，但对短文本（比如推特）的效果还不清楚。凭我的直觉，对短文本使用预训练的词向量应该能比长文本取得更好的效果。</p>
<p>搭建一个CNN模型结构需要选择许多个超参，上文中已经提到一些：输入的特征（word2vec, GloVe, one-hot），卷积滤波器的数量和尺寸，池化策略选择（最大值、平均值）以及激活函数（ReLU, tanh）。文献[7]通过多次重复实验，比较了不同超参在性能和稳定性方面对CNNs模型结构的影响。如果你想自己实现一个CNN文本分类模型，借鉴该论文的结果会是一个挺好的选择。其实验结论主要有：1）最大池化效果总是优于平均池化；2）选择理想的滤波器尺寸很关键，但也视情况而定；3）正则化处理在NLP任务中的效果并不明显。对于该结论需要注意的是，该研究所用文本集里的文本长度都相近，若是要处理长度不同的文本集，上述结论可能不具有借鉴意义。</p>
<p>文献[8]探索了CNNs在关系挖掘（Relation Extraction）和关系分类（Relation Classification）任务中的应用。除了词向量特征，作者还把词与词的相对位置信息作为卷积层的输入。这个模型假设了所有文本元素的位置已知，每个输入样本只包含一种关系。文献[9]和文献[10]使用的模型类似。</p>
<p>微软研究院的研究成果：文献[11]和 [12]介绍了CNNs在NLP的另一种有趣的应用。这两篇论文介绍了如何学习将句子表示成包含语义的结构，它能被用来做信息检索。论文中给出的例子是基于用户当前的阅读内容，为其推荐其它感兴趣的文档。句子的表征是基于搜索引擎的日志数据训练得到的。</p>
<p>大多数CNN模型以这样或是那样的训练方式来学习单词和句子的词向量表征，它是训练过程的一部分。并不是所有文献都关注这一步训练过程，也并不在乎学到的表征意义有多大。文献[13]介绍了用CNN模型对Facebook的日志打标签。这些学到的词向量随后又被成功应用于另一个任务 —— 基于点击日志给用户推荐感兴趣的文章。</p>
<h3 id="字符层级的CNNs模型"><a href="#字符层级的CNNs模型" class="headerlink" title="字符层级的CNNs模型"></a>字符层级的CNNs模型</h3><p>到目前为止，上面讨论的模型表征还都停留在单词层级。但是如何将CNNs模型直接用于字符也有一些相应研究。文献[14]通过学习字符层级的向量表征，结合预训练的词向量，以此达到给语音打标签的目的。文献[15]和[16]不依赖预训练词向量的这一步，研究用CNNs模型直接从字符学习。值得注意的是，这些研究作者使用了相对较深层的网络结构，达到了9层，以此完成语义分析和文本分类任务。结果显示，用字符层级的输入在大规模数据集（百万级）上学习的效果非常好，但在简单模型和小数据集（十万级）上的学习效果却是一般。文献[17]是关于字符级卷积运算在语言建模方面的应用，将字符级CNN模型的输出作为LSTM模型每一步的输入。同一个模型用于不同的语言。</p>
<p>令人惊讶的是，上面所有论文几乎都是发表于近一到两年。显然CNNs模型在NLP领域已经有了出色的表现，新成果和顶级系统还在层出不穷地出现。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><br><li>[1]&nbsp;<a href="http://arxiv.org/abs/1408.5882" onclick="javascript:window.open('http://arxiv.org/abs/1408.5882'); return false;" target="_blank" rel="external">Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751.</a></li><br><li>[2]&nbsp;<a href="http://arxiv.org/abs/1404.2188" onclick="javascript:window.open('http://arxiv.org/abs/1404.2188'); return false;" target="_blank" rel="external">Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665.</a></li><br><li>[3]&nbsp;<a href="http://www.aclweb.org/anthology/C14-1008" onclick="javascript:window.open('http://www.aclweb.org/anthology/C14-1008'); return false;" target="_blank" rel="external">Santos, C. N. dos, &amp; Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78).</a></li><br><li>[4]&nbsp;<a href="http://arxiv.org/abs/1412.1058v1" onclick="javascript:window.open('http://arxiv.org/abs/1412.1058v1'); return false;" target="_blank" rel="external">Johnson, R., &amp; Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011).</a></li><br><li>[5]&nbsp;<a href="http://arxiv.org/abs/1504.01255" onclick="javascript:window.open('http://arxiv.org/abs/1504.01255'); return false;" target="_blank" rel="external">Johnson, R., &amp; Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding.</a></li><br><li>[6]&nbsp;<a href="http://www.aclweb.org/anthology/P15-2058" onclick="javascript:window.open('http://www.aclweb.org/anthology/P15-2058'); return false;" target="_blank" rel="external">Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., &amp; Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357.</a></li><br><li>[7]&nbsp;<a href="http://arxiv.org/abs/1510.03820" onclick="javascript:window.open('http://arxiv.org/abs/1510.03820'); return false;" target="_blank" rel="external">Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification,</a></li><br><li>[8] <a href="http://www.cs.nyu.edu/~thien/pubs/vector15.pdf" onclick="javascript:window.open('http://www.cs.nyu.edu/~thien/pubs/vector15.pdf'); return false;" target="_blank" rel="external">Nguyen, T. H., &amp; Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48.</a></li><br><li>[9]&nbsp;<a href="http://ijcai.org/papers15/Papers/IJCAI15-192.pdf" onclick="javascript:window.open('http://ijcai.org/papers15/Papers/IJCAI15-192.pdf'); return false;" target="_blank" rel="external">Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., &amp; Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339.</a></li><br><li>[10]&nbsp;<a href="http://www.aclweb.org/anthology/C14-1220" onclick="javascript:window.open('http://www.aclweb.org/anthology/C14-1220'); return false;" target="_blank" rel="external">Zeng, D., Liu, K., Lai, S., Zhou, G., &amp; Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344.&nbsp;</a></li><br><li>[11]&nbsp;<a href="http://research.microsoft.com/pubs/226584/604_Paper.pdf" onclick="javascript:window.open('http://research.microsoft.com/pubs/226584/604_Paper.pdf'); return false;" target="_blank" rel="external">Gao, J., Pantel, P., Gamon, M., He, X., &amp; Deng, L. (2014). Modeling Interestingness with Deep Neural Networks.</a></li><br><li>[12] <a href="http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf" onclick="javascript:window.open('http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf'); return false;" target="_blank" rel="external">Shen, Y., He, X., Gao, J., Deng, L., &amp; Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110.&nbsp;</a></li><br><li>[13] <a href="http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf" onclick="javascript:window.open('http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf'); return false;" target="_blank" rel="external">Weston, J., &amp; Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827.</a></li><br><li>[14]&nbsp;<a href="http://jmlr.org/proceedings/papers/v32/santos14.pdf" onclick="javascript:window.open('http://jmlr.org/proceedings/papers/v32/santos14.pdf'); return false;" target="_blank" rel="external">Santos, C., &amp; Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826.&nbsp;</a></li><br><li>[15]&nbsp;<a href="http://arxiv.org/abs/1509.01626" onclick="javascript:window.open('http://arxiv.org/abs/1509.01626'); return false;" target="_blank" rel="external">Zhang, X., Zhao, J., &amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9.</a></li><br><li>[16]&nbsp;<a href="http://arxiv.org/abs/1502.01710" onclick="javascript:window.open('http://arxiv.org/abs/1502.01710'); return false;" target="_blank" rel="external">Zhang, X., &amp; LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102.</a></li><br><li>[17]&nbsp;<a href="http://arxiv.org/abs/1508.06615" onclick="javascript:window.open('http://arxiv.org/abs/1508.06615'); return false;" target="_blank" rel="external">Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2015). Character-Aware Neural Language Models.</a></li><br></ul>

</div><div class="tags"><a href="/tags/卷积神经网络/">卷积神经网络</a><a href="/tags/自然语言处理/">自然语言处理</a><a href="/tags/NLP/">NLP</a><a href="/tags/机器学习/">机器学习</a><a href="/tags/翻译/">翻译</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/09/22/Understanding_LSTM_Networks/" class="pre">深入理解长短期记忆模型</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是卷积神经网络"><span class="toc-text">什么是卷积神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何将CNN应用到NLP？"><span class="toc-text">如何将CNN应用到NLP？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN模型的超参数"><span class="toc-text">CNN模型的超参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#窄卷积和宽卷积"><span class="toc-text">窄卷积和宽卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#步长"><span class="toc-text">步长</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#池化层"><span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#通道"><span class="toc-text">通道</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN在自然语言处理中应用"><span class="toc-text">CNN在自然语言处理中应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#字符层级的CNNs模型"><span class="toc-text">字符层级的CNNs模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/05/neo4j-whitespace/">neo4j记录日志-带有连续空格的cql语句执行问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_Convolutions/">深入理解卷积</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_LSTM_Networks/">深入理解长短期记忆模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_Convolutional_Neural_Networks_for_NLP/">卷积神经网络在自然语言处理中的应用</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/">神经网络</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/翻译/">翻译</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/翻译/机器学习/">机器学习</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/机器学习/">机器学习</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/机器学习/翻译/">翻译</a><span class="category-list-count">1</span></li></ul></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/neo4j/" style="font-size: 15px;">neo4j</a> <a href="/tags/知识图谱/" style="font-size: 15px;">知识图谱</a> <a href="/tags/卷积/" style="font-size: 15px;">卷积</a> <a href="/tags/卷积神经网络/" style="font-size: 15px;">卷积神经网络</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/自然语言处理/" style="font-size: 15px;">自然语言处理</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/翻译/" style="font-size: 15px;">翻译</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">塞外烟雨.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>