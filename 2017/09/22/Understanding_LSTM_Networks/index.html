<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>深入理解长短期记忆模型 | 塞外烟雨</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深入理解长短期记忆模型</h1><a id="logo" href="/.">塞外烟雨</a><p class="description">假装会NLP和KG的攻城狮</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">深入理解长短期记忆模型</h1><div class="post-meta"><a href="/2017/09/22/Understanding_LSTM_Networks/#comments" class="comment-count"></a><p><span class="date">Sep 22, 2017</span><span><a href="/categories/神经网络/" class="category">神经网络</a><a href="/categories/神经网络/机器学习/" class="category">机器学习</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>博客原文 [<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>)</p>
<h2 id="循环神经网络简介（RNN）"><a href="#循环神经网络简介（RNN）" class="headerlink" title="循环神经网络简介（RNN）"></a>循环神经网络简介（RNN）</h2><p>人们通常并不是每次都是从头开始思考问题，就如你在阅读本文时，你所理解的词语的含义是建立在前面的词语基础上的。你并不是把所有的历史信息都丢弃而从头开始理解，换言之，你的的思想是连续的。</p>
<p>传统的神经网络并没有克服这一点，这也看起来是它的巨大劣势。 例如，当你想区分在一部电影的每一个时刻发生了什么，如果用传统的神经网络，将无法利用已经发生的先验信息来理解后续的画面。</p>
<p>循环神经网络（Recurrent neural networks，RNN）克服了这些弊端。它们具有神经元循环作用，使得历史信息能够得以保留、延续。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/RNN-rolled.png" width="100" height="60"><br></div>

<p>循环神经网络结构</p>
<p>在上面的网络结构中，神经网络的一部分$A$，能够在$t$时刻输入 \(x_t\)，得到输出值 \(h_t\)。循环结构能够让从上一时刻的输出值作为下一次时刻的输入值。 </p>
<p>这些循环结构让循环神经网络看起来有些神秘。然而，当你在思考下会发现，RNN和一般的神经网络并不是完全不同。一个循环神经网络可以看成是同一个网络的多个复制，每个网络都携带信息传入到下一个继承者。我们将RNN进行展开，可以得到：</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/RNN-unrolled.png" width="500" height="400"><br></div>

<p>一个展开的RNN</p>
<p>这个链状的神经网络显示了，RNN 和序列紧密联系。这种特性就是用来处理序列数据的。</p>
<p>当然，它也确实被用来处理这类数据。在过去的几年，RNNs在许多问题上都取得了不可思议的成果：语音识别，语言模型，翻译，图像标签等。我将不再过多讨论RNNs取得成功案例，更多可以参看 Andrej Karpathy’s 的优秀的博客<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
<p>RNNs能够取得这些成功的主要因素还是 “LSTMs”的广泛使用，作为一种特殊的RNN，LSTM在很多任务上都比一般的RNN表现出色。几乎所有的在RNNs上的结构，都能够用LSTM很好的完成，这些LSTM就是本文要讨论的。</p>
<h2 id="长期依赖的问题"><a href="#长期依赖的问题" class="headerlink" title="长期依赖的问题"></a>长期依赖的问题</h2><p>RNNs的一个启发思想是它能够将先前的信息保留用于当前的任务中，例如，利用先前的图像能够帮助理解当前画面。 如果RNNs能够达到这个要求，那么将会大有所用。事实上它也确实做到了。</p>
<p>有时候，我们仅需要关注最近的信息就足够完成当前的任务。例如，考虑用前面的单词来预测下一个单词的语言模型。如果我们想预测“the clouds are in the sky,”  的最后一个单词，我们不需要其他前面的上下文信息。很明显，句子的下一个单词就是sky。在这个例子中，相关的信息和需要的信息差异比较小，RNNs模型能够学习利用前文的信息。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/RNN-shorttermdepdencies.png" width="500" height="400"><br></div>

<p>但还是有很多情况，我们需要更多的上下文。设想你想预测 “I grew up in France… I speak fluent French.” 的下一个单词。临近的的信息表明，下一个单词是一个语言名称，但是如果我们想缩小语言范围，我们需要 France的上下文。极大可能的是，我们需要的信息和相关的信息差异很大。 </p>
<p>不幸的事，当这个差异越来越大时，RNNs对链接这些信息开始变得无力。 </p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/RNN-longtermdependencies.png" width="500" height="400"><br></div>

<p>神经网络在长信息依赖情况表现的很挣扎。</p>
<p>理论上，RNNs模型是绝对能够处理这类  “长期依赖”的问题。可以仔细选择参数来解决这样的问题。 可惜的是，在实践中，RNNs看起来并不总是能够学习到这些信息。这些问题在 Hochreiter (1991) [German] and Bengio, et al. (1994) 有更多的讨论， 并且提到了一些可能是导致这些困难的根本原因。</p>
<p>幸运的是，LSTMs模型能够处理这些问题！</p>
<h2 id="LSTM神经网络"><a href="#LSTM神经网络" class="headerlink" title="LSTM神经网络"></a>LSTM神经网络</h2><p>长短时间记忆模型 – 通常被叫做 “LSTMs” – 是一种特殊的RNN，能够学习到长期依赖。由 Hochreiter &amp; Schmidhuber (1997)提出，并且在以下文献得到优化和推广。这些模型在很多情况下表现的出乎意料的好 ，如今也被广泛应用。</p>
<p>LSTMs 被主要设计是用来解决长期依赖的问题。记忆长期的信息是基本的模型特性，而不是很困难的才能学习到这些信息。</p>
<p>最近所有的神经网络都有很多复制网络的链式形式。 在标准的 RNNs，这种复制都有比较简单的结构，例如单个 tanh 层。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-SimpleRNN.png" width="500" height="400"><br></div> 

<p>LSTMs 也有类似链状的结构，但是复制的模块有不同的结构。不是一个单一的神经网络层，他有四个，并且用特别的方式来交互。 </p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-chain.png" width="500" height="400"><br></div>

<p>LSTM中复制的模块包含四个交互的层。</p>
<p>不用太在意细节。我们会一步一步的来拆解LSTM。 现在，我们先熟悉后续需要用到的符号。 </p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM2-notation.png" width="500" height="400"><br></div>

<p>在上图，实线代表向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐点的运算，如向量加法。黄色框是学习神经网络层。实线合并表示串联合并。实线分岔表示其内容正在复制，副本将转到不同的位置。</p>
<h2 id="LSTMs背后的核心思想"><a href="#LSTMs背后的核心思想" class="headerlink" title="LSTMs背后的核心思想"></a>LSTMs背后的核心思想</h2><p>LSTMs的关键是cell state，水平线穿过图标的顶部。cell state 就像一个传送带，但是他直接穿过整个链条，只有一些小的线性相互作用。这让信息可以流畅地传递下去且保持不变。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-C-line.png" width="700" height="600"><br></div>

<p>LSTM能够移除或者相加信息到cell state，通过阀门来有效的控制。</p>
<p>阀门是一个让信息穿过的选择方式。由 sigmoid 网络层和点乘运算组成。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-gate.png" width="200" height="100"><br></div>

<p> sigmoid 层生成0到1之间的数字，体现了有多大的可能性让能通过。0意味着 “全不通过” 1意味着 “全都通过”。</p>
<p>一个LSTM存在三种这样的阀门，为了保护和控制cell state。</p>
<h2 id="一步一步理解LSTM"><a href="#一步一步理解LSTM" class="headerlink" title="一步一步理解LSTM"></a>一步一步理解LSTM</h2><p>LSTM的第一步是决定了哪些信息能够从cell state丢弃。这些决定是被  sigmoid 层 被称作“遗忘门层” 。它接受\(h_{t-1}\) and \(x_t\),，并且在 cell state \(C_{t-1}\)输出一个 \(0\) 到 \(1\) 的数。\(1\) 表示 “全保留”当\(0\) 表示 “全丢弃”。</p>
<p>再次回到之前用上文来预测单词的语言模型。在这个例子中，cell state可能包括当前受试者的性别，从而可以使用正确的代词。当我们看到一个新主题时，我们想要忘记这个老主题的性别。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-focus-f.png" width="800" height="700"><br></div>

<p>下一步是决定存储哪些新的信息在cell state。 这有两部分， 第一步，  sigmoid 层被称之为 “输入门层” 决定了哪些值会更新。第二， tanh 层生成了一个新的候选值向量 \(\tilde{C}_t\)， 这将会被加到state。下一步，我们将会整合这两部分的内容更新到state.</p>
<p>在我们的语言模型中，我们将新对象的性别添加到cell state中，以替换我们将要忘记的旧对象。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-focus-i.png" width="800" height="700"><br></div>

<p> 现在可以更新旧的 cell state  \(C_{t-1}\)到新的 cell state \(C_t\)。之前的步骤已经决定了哪些会做，我只是完成而已。</p>
<p>通过 \(f_t\)乘以旧的 state,，来忘记之前决定要忘记的信息。然后会加上 \(i_t*\tilde{C}_t\)。这是一个新的候选值，被我们想多大程度的进行放缩来更新值。</p>
<p>在语言模型的情况下，这个步骤就是用来放弃关于旧主题性别的信息，并添加新信息。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-focus-C.png" width="800" height="700"><br></div>

<p> 最后，我们需要决定我们需要输出什么。输出的信息是依赖 cell state而定的，但是一个过滤掉的版本。第一，我们会通过 sigmoid 层决定那部分的 cell state 会作为输出。然后，将 cell state 通过 \(\tanh\) (to push the values to be between \(-1\) 和 \(1\)) 和 相乘 通过输出的 sigmoid 门，以便得到我们想要的输出信息。</p>
<p> 例如语言模型例子，当看到一个对象，他可能希望我们输出一个关于动词的信息，这就是下一个实现的。例如，它可能会输出对象是单数还是复数，以便我们知道动词应该如何组合在一起，如果这是接下来的内容。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-focus-o.png" width="800" height="700"><br></div>

<h2 id="LSTM的一些变体"><a href="#LSTM的一些变体" class="headerlink" title="LSTM的一些变体"></a>LSTM的一些变体</h2><p>前面讨论的都是一个很普通的LSTM。并不是所有的  LSTMs 都和上面一样。事实上，几乎所有的论文在涉及到LSTM都会提到不同的版本。这些差异都很小，但有些值得关注。</p>
<p>一个比较流行的LSTM辩题，由Gers &amp; Schmidhuber (2000) 提出，就是添加  “peephole connections.” 。意味着他让我们的  gate layers 接受来自cell state的数据。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-var-peepholes.png" width="800" height="700"><br></div>

<p>上面的图形增加 peepholes到所有的门，但是许多文章只使用了部分 peepholes。</p>
<p>另外一变体，就是利用耦合遗忘和输入门。而不是单独的决定哪些遗忘和哪些更新信息，将这些决定综合起来。只当我们要在输入内容时忘记。当我们忘记旧的东西时，我们只会向cell state中输入新的值。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-var-tied.png" width="800" height="700"><br></div>

<p>一个有趣的 LSTM 变体是 Gated Recurrent Unit，或者叫 GRU，由Cho, et al. (2014) 提出。他这个遗忘和输入门成单一的 “更新门” 。同时合并了 cell state 和 hidden state，以及其他的一些改变。所得的模型比一般的LSTM还要简单，现在已经变得越来越流行。</p>
<div align="center"><br> <img src="/img/Understanding_LSTM_Networks/LSTM3-var-GRU.png" width="800" height="700"><br></div>

<p>上面只是少数值得关注的 LSTM的变体。还有其他的，例如 Depth Gated RNNs by Yao, et al. (2015)。还有一些完全不同的处理长期依赖的方法，例如 Clockwork RNNs by Koutnik, et al. (2014)。</p>
<p>但是哪些变体是最好的呢？或者这些差异明显吗？ Greff, et al. (2015) 做了一个比较，发现他们都差不多。 Jozefowicz, et al. (2015) 测试了超过万次的 RNN architectures，发现又一些在一个特殊的任务下是优于LSTMs的。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>前面，我提到了一些RNNs取得的一些非凡的进展。几乎所有的这些都能够利用 LSTMs 得到一样的结果。在有些任务上表现的更好！</p>
<p>如果写下一组公式， LSTM 看起来十分不友好。通过本文一步一步理解，能够使得LSTM更容易接受。</p>
<p>LSTMs 是优化RNNs成功的一大步。理所当然会想到：是否还有其他方式呢？在研究者的共同观点是“是的，有下一步，且值得关注！” 。主要思想是RNN的每一步都提取跟多的信息。例如，你想用RNN来给图像来创建一个标签，可能需要提取图片的每一部分来寻找输出的所有单词。事实上， Xu, et al. (2015) 却是这样做的 – 可能是一个有趣的起点！有一些真正令人兴奋的结果使用注意力，似乎还有更多的是在角落…</p>
<p>注意力不是RNN研究中唯一令人兴奋的线索。例如，  由 Kalchbrenner, et al. (2015) 提出的Grid LSTMs，似乎很令人振奋。在 generative 模型中也有用到RNNs –例如 Gregor, et al. (2015), Chung, et al. (2015), 和 Bayer &amp; Osendorfer (2015) – 也很有趣。 最近几年对于卷积神经网络来说已经是一个激动人心的时刻了，下一个希望只有更加如此！</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢许多朋友帮助我更好的理解 LSTMs，评论可视化，和关于本博客提供反馈。</p>
<p>非常感谢我的google的同事的帮助，尤其是 Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever。也十分感谢我的朋友和同事抽出时间帮助我。 包括 Dario Amodei, and Jacob Steinhardt。特别感谢  Kyunghyun Cho for extremely 来对图形的支持。</p>
<p>在博文之前，我解释了  LSTMs 在两个 seminar series 我分享 neural networks。感谢所有协同的人。</p>
<p>还有原作者，需要对LSTM模型做出贡献的人。一个不完全的名单列表：Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves。</p>
</div><div class="tags"><a href="/tags/LSTM/">LSTM</a><a href="/tags/神经网络/">神经网络</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2018/04/05/neo4j-whitespace/" class="pre">neo4j记录日志-带有连续空格的cql语句执行问题</a><a href="/2017/09/22/Understanding_Convolutional_Neural_Networks_for_NLP/" class="next">卷积神经网络在自然语言处理中的应用</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络简介（RNN）"><span class="toc-text">循环神经网络简介（RNN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#长期依赖的问题"><span class="toc-text">长期依赖的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM神经网络"><span class="toc-text">LSTM神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTMs背后的核心思想"><span class="toc-text">LSTMs背后的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一步一步理解LSTM"><span class="toc-text">一步一步理解LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM的一些变体"><span class="toc-text">LSTM的一些变体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结论"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#致谢"><span class="toc-text">致谢</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/05/neo4j-whitespace/">neo4j记录日志-带有连续空格的cql语句执行问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_LSTM_Networks/">深入理解长短期记忆模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_Convolutional_Neural_Networks_for_NLP/">卷积神经网络在自然语言处理中的应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/Understanding_Convolutions/">深入理解卷积</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/">神经网络</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/翻译/">翻译</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/神经网络/翻译/机器学习/">机器学习</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/机器学习/">机器学习</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/机器学习/翻译/">翻译</a><span class="category-list-count">1</span></li></ul></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/neo4j/" style="font-size: 15px;">neo4j</a> <a href="/tags/知识图谱/" style="font-size: 15px;">知识图谱</a> <a href="/tags/自然语言处理/" style="font-size: 15px;">自然语言处理</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/卷积神经网络/" style="font-size: 15px;">卷积神经网络</a> <a href="/tags/翻译/" style="font-size: 15px;">翻译</a> <a href="/tags/卷积/" style="font-size: 15px;">卷积</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">塞外烟雨.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>