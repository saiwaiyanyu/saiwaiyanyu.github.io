[{"title":"neo4j记录日志-带有连续空格的cql语句执行问题","date":"2018-04-04T23:48:41.000Z","path":"2018/04/05/neo4j-whitespace/","text":"背景在创建CONSTRAINT发现创建失败，但是创建的PROPERTY提前已经做了UNIQUE处理，因此查找原因，发现提示不是UNIQUE，分析之后发现，CQL在执行过程中，有时候多个连续空格的STRING 居然会识别错误，因此记录下。 1234567// test.cqlMATCH (N) DETACH DELETE N;RETURN LENGTH('1 1'),LENGTH('1 1'),LENGTH('1 1');RETURN LENGTH('1 1') ; RETURN LENGTH('1 1') ; RETURN LENGTH('1 1') ; RETURN LENGTH('1'),LENGTH('11'),LENGTH('111'),LENGTH('1111') ; 1neo4j-shell -file test.cql 123456789101112131415161718192021222324252627282930313233343536373839 +--------------------------------------------+| No data returned, and nothing was changed. |+--------------------------------------------+3 ms+--------------------------------------------------+| LENGTH(&apos;1 1&apos;) | LENGTH(&apos;1 1&apos;) | LENGTH(&apos;1 1&apos;) |+--------------------------------------------------+| 3 | 4 | 5 |+--------------------------------------------------+1 row9 ms+---------------+| LENGTH(&apos;1 1&apos;) |+---------------+| 3 |+---------------+1 row7 ms+---------------+| LENGTH(&apos;1 1&apos;) |&lt;&lt;&lt;&lt;&lt;错误+---------------+| 3 |+---------------+1 row4 ms+---------------+| LENGTH(&apos;1 1&apos;) |&lt;&lt;&lt;&lt;&lt;错误+---------------+| 3 |+---------------+1 row4 ms+-------------------------------------------------------------+| LENGTH(&apos;1&apos;) | LENGTH(&apos;11&apos;) | LENGTH(&apos;111&apos;) | LENGTH(&apos;1111&apos;) |+-------------------------------------------------------------+| 1 | 2 | 3 | 4 |+-------------------------------------------------------------+1 row10 ms 结果可以看出 123RETURN LENGTH('1 1') ; RETURN LENGTH('1 1') ; RETURN LENGTH('1 1') ; 三条返回结果，对空格的识别失败，具体cql的执行逻辑还需要再深入研究下。后续再更新。","tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"},{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/tags/知识图谱/"}]},{"title":"卷积神经网络在自然语言处理中的应用","date":"2017-09-22T12:28:55.000Z","path":"2017/09/22/Understanding_Convolutional_Neural_Networks_for_NLP/","text":"博客原文 Understanding Convolutional Neural Networks for NLP 当听到卷积神经网络 (Convolutional Neural Network ,CNNs) 的时候， 我们常会联想到计算机视觉。CNNs 是图像分类领域取得重大突破的一个重要因素。如今，从 Facebook 图像自动标签应用到自动驾驶领域，CNNs 俨然成了计算机视觉系统的关键技术。 近期，我们已经着手尝试将 CNNs 应用到自然语言处理（Natural Language Processing，NLP），并且已经取得一些初步进展。我将在本文中归纳，什么是CNNs？以及如何将其应用到NLP？从计算机视觉的用例出发，更容易理解 CNNs背后的逻辑，因此，我们先从这里开始，然后逐渐过渡到NLP。 最简洁、直观的理解卷积（Convolution），就是把它想象成应用到矩阵的滑动窗口函数。这种表述比较口语化了，我们用一个动画来表示就更直观了： *3x3的滤波器做卷积运算。图片来源： http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution 左侧的矩阵可以想象成是一个黑白图像的表征， 每一个元素都表示一个像素点，0-表示黑点，1-表示白点（灰度的图像的像素值范围一般是0-255）。滑动窗口又称作核、滤波器或是特征检测器。这里我们使用3x3的滤波器，将滤波器和矩阵对应的部分逐个元素相乘，并求和。通过平移窗口，使滤波器滑过矩阵的所有像素，可以对整个图像做卷积运算。 也许你会疑问，通过上述操作，我们能够得到什么呢？以下是几个直观的例子： 用图像像素及其邻近的像素平均值，达到模糊图像的效果。 用邻近点像素值与自身的差值替换其原值，实现边缘检测的效果 GIMP手册里还其他的例子。如果想要深入了解卷积运算的原理，我推荐阅读Chris Olah的博客。 什么是卷积神经网络现在你应该知道什么是卷积了。但是什么是卷积神经网络（CNNs）又是什么呢？CNNs其本质是多层卷积的叠加运算，并结合类似ReLU或tanh等非线性激活函数的结果。 在传统的前馈神经网络（feedforward neural network）中，我们把每一个输入神经元和下一层的输入神经元相连。这种处理方式也叫做全连接层（fully connected layer），或者仿射层（affine layer）。 在CNNs处理时，我们并不采用这样的方式，而是采用输入层的卷积结果来计算输出。这等效于于是局部连接（local connections），每个局部的输入区域与输出的一个神经元相连接。对每一层应使用同的滤波器，滤波器通常是如上图所示的成百上千个，然后汇总它们的结果。这里也涉及到池化层（降采样）（pooling (subsampling) layers），我会在后文做解释。在训练阶段，CNN基于你想完成的任务自动学习滤波器的权重值。举个例子，在图像分类问题中，第一层CNN模型或许能学会从原始像素点检测到一些边缘线条，然后根据边缘线条在第二层检测出一些简单的形状，然后基于这些形状检测出更高级的特征，比如脸部轮廓等。最后一层是利用这些高级特征的一个分类器。 这种计算方式时，有两点值得注意: 位置不变性（Location Invariance）和组合性（Compositionality）。例如你想识别一张图片里面是否包含大象，因为滤波器是在整个图片上滑动，因此，并不关心大象在图片的具体位置。事实上，池化处理也有助于平移、旋转和缩放的不变性，它对克服缩放因素的效果尤其好。 第二个关键点是（局部）组合性（(local) compositionality）。每个滤波器对一小块局部区域的低级特征组合形成更高级的特征表示。这也是CNNs对计算机视觉作用巨大的原因。我们可以很直观地理解，线条由像素点构成，基本形状又由线条构成，更复杂的物体又源自基本的形状。 如何将CNN应用到NLP？不同于图像的像素，大部分NLP的处理的任务是通过矩阵来表示的句子或者文档。矩阵的每一行表示一个词元素（token），例如是一个词，或者是一个字符。也即是说，每行都是表示一个词的向量。通常，这些向量是 word embeddings (low-dimensional representations) 例如 word2vec 和 GloVe，也可以是one-hot 向量，即是词在词表（vocabulary）中的索引（index）位置。对于一个19个词的句子，用100-维的enbedding表示，能够得到10×100的矩阵，这就是我们得到的“图像”。 在计算机视觉处理时，滤波器会滑过图片的局部区域（ local patches），但是在NLP时，一般是利用滤波器滑过矩阵的所有行（词）。因此，滤波器的“宽度”一般是输入矩阵的宽度，高度或者区域大小，可以调节，但是一次滑动2-5个词是比较常见的做法。综上，CNNs用来处理NLP的过程可以看成这样： 应用于文本句子分类的 CNNs 结构示意图。我们描述三种尺寸的滤波器: 2, 3 和 4, 美中尺寸都有 2 种滤波器。每个滤波器都对文本句子矩阵进行卷积运算操作，得到特征词典。然后在每个特征字典上做最大池化处理，也即是记录下每个特征字典的最大。这样，从所有的6个特征字典生成得到一个单变量特征向量（univariate feature vector），并且这6个特征拼接成一个特征向量，传入神经网络的倒数第二层 （penultimate layer）。最终softmax层接收这个特征向量作为输入，并用作句子分类。我们假设这是一个二分类问题，最终输出就是两个可能的状态（state） 。图片来源: Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification. 到此，是否已有计算机视觉的直观感觉？局部不变性（Location Invariance ）和局部组合性（local Compositionality）对图像计算很直观，但是对于NLP且不失如此。也许你会关注到单词在句子中出现的位置，像素和相邻的点可能是相关联的，（都是同一物体的一部分），但是对于单词却不尽然。在许多种语言里, 短语会被多个词语分割。组合型也不是那么显然， 显然，单词会以某种方式组合在一起，例如形容词用做修饰名词，但是，想了解这些高级特征的真正的“含义”，却不如计算机视觉那么直观了。 综上看来，好像CNNs并不能很好的处理NLP任务。 递归神经网络（Recurrent Neural Networks，RNN）看起来更直观。它们模仿人类处理语言的逻辑方式（至少是我们认为我们处理的方式） ：从左往右阅读。 幸运的是, 这并不意味CNNs就无效了。如果不能被正确的使用，那么所有的模型都是错的。事实证明，CNNs用来处理NLP表现非常好。词袋模型（Bag of Words model ）是一个基于错误假设的简化模型，但是这并不妨碍它成为多年处理NLP的标准方式，并且表现良好。 CNNs的一个主要特点就是训练速度快，非常快。 卷积运算是计算机图形的核心部分，并在GPUs的硬件层实现。相比于n-gram，CNNs 在表征方式上也十分高效。在一个超大的词典上，计算任何超过3-grams的都会非常大的开销。即使是google也没法在超过5-ngrams上进行处理。在不需要表征全量词典的条下，卷积滤波器（Convolutional Filters）会自动学习较好的表征方式。选择尺寸超过5的滤波器也是完全合理的。 个人认为，在底层学到的滤波器能够获取到n-grams想尽的特征，尽管这些特征是以更加紧凑的方式表征。 CNN模型的超参数在谈到如何用CNNs来处理NLP任务前，可以先看一下在构建CNN模型时需要面临的几个选择。希望这些内容能够帮助你更好的理解该领域相关的文献。 窄卷积和宽卷积在前文解释什么是卷积时，我忽略了使用滤波器时的一个小细节。在矩阵中间使用 3×3 的滤波器没有问题，但是在矩阵边缘该如何处理呢？矩阵的第一个元素，在其左面和上面没有临近元素，我们该如何应用滤波器呢？可以采用补零法 （zero-padding），所有超过矩阵的外部元素都取值0。通过这样方式，就可以对输入的矩阵的任何元素都能够应用滤波器了，并且得到一个和输入矩阵同样大小或者更大的结果矩阵。 补零法又叫做宽卷积，不使用补零则是窄卷积。 一个1D的例子如下: 窄卷积 vs. 宽卷积。滤波器大小 5，输入大小 7。来源: A Convolutional Neural Network for Modelling Sentences (2014) 可以看出，当滤波器的长度和输入比较接近时，宽卷积是很有用，甚至说是很有必要的。以上，窄卷积的输出大小是 (7-5) + 1=3，而宽卷积的输出则是 (7+2*4 - 5) + 1 =11。一般地，输出形式可以表示为： 步长卷积运算的另一个超参数是步长，即滤波器每滑动一次的距离。上面示例中的步长都是1，且相邻的滤波器有重叠。步长越大，则用到的滤波器越少，输出的值也越少。下图来自斯坦福的cs231课程网页，分别是步长为1和2的情况： 卷积步长。左侧：步长为1，右侧：步长为2。来源： http://cs231n.github.io/convolutional-networks/ 在文献中我们常常见到的步长是1，但选择更大的步长会让模型更接近于递归神经网络，其结构就像是一棵树。 池化层池化层是卷积神经网的一个重要概念，一般应用到卷积层之后。 池化层是对输入的降采样（subsample）。池化处理的一般做法是对每个滤波器的输出结果进行最大化。并不需要对全量矩阵进行池化，对其一个窗口进行池化即可。例如，对2x2的窗口进行最大池化（在NLP时，一般是对全量输出进行池化，对每一个滤波器结果得到唯一的值）： CNN 的最大池化处理. Source: http://cs231n.github.io/convolutional-networks/#pool 为什么要池化处理呢？有多种原因。池化的的一个特点就是输出一个固定大小的矩阵，这是分类问题所需要的。例如，如果有1,000个滤波器，并且对每个都进行最大化处理，那么你会得到一个1000-维德输出，无论给定过的滤波器大小是什么，或者你输入大小。这让你能够可变长度的句子，可变大小的滤波器，但是最终都能得到一样维度的输出，进而传入分类器。 池化处理也能够对输出进行降维，并（理想情况）尽可能的保留有用信息。可以想象成，每一个滤波器都探测一种特殊的特征，例如探测句子是否包含一个负面的特征“not amazing”。如果类似短语在句子的某个位置出现，那么滤波器的在该区域的将会生成一个较大的值，在其他区域则是较小的值。通过最大处操作，能够保留这些特征是否出现在句子的信息，但是会丢失出现的具体位置的信息。但是具体出现具体位置真的有用吗？是的，这就是类似一组n-gram所做的工作。尽管丢了关于位置信息的全局信息（句子的大致位置），但是滤波器保留了局部的信息，例如“not amazing” 和“amazing not”的含义就相差甚远。 在图像识别中，池化还能够具有平移和旋转不变性。当你在一个区域进行池化，即使对徒刑进行了旋转／平移操作，所得到的结果基本一样，因为最大化处理得到的结果总是一样的。 通道最后一个需要了解的概念是通道。通道是输入数据的不同“视角”。例如，图像识别时一般会有RGB（红，绿，蓝）通道。当赋予不同或相同的权值时，可以在通道上做卷积运算。在NLP处理时，也可以想象有不同的通道：对于不同的word embeddings (word2vec 和 GloVe )，你可以有一个特殊的通道 , 或者你可得到一个通道，对句子通过不同的语言表示，或者短语不同的表达。 CNN在自然语言处理中应用现在我们看看CNNs如何应用到自然语言处理。我会总结一下该方面的研究成果，我希望能够概括最近的比较流行的研究成果，尽管会遗漏一些有趣的应用。 对于CNNs最自然的一个应用就是分类任务。例如情感分析，垃圾邮件识别，主题归类，卷积和池化运算会丢失一些局部的词语的顺序信息，因此对于句子序列标记任务或者实体抽取任务对于CNNs来说难度较大，（当然也不是不可能，你可以在输入时增加位置特征）。文献【1】对于评估CNNs主要是在多分类任务上，特别是语义分析和主体分类任务。CNNs在数据集分类上表现不错，有些还刷新了记录。但是出乎意料的是，本文采用的神经网络很简单，但是却取得较好成果。输入成是将句子组合成的word2vec的词向量，接着是由若干个滤波器组成的卷积层，然后是最大池化层，最后是softmax分类器。该论文也尝试了两种不同形式的通道，分别是静态和动态词向量，其中一个通道在训练时动态调整而另一个不变。文献[2]中提到了一个类似的结构，但更复杂一些。文献[6]在网络中又额外添加了一个层，用于语义聚类。 Kim, Y. (2014). 卷积神经网络用来语句分类 文献[4]从原始数据训练CNN模型，不需要预训练得到word2vec或GloVe等词向量表征。它直接对one-hot向量进行卷积运算。作者对输入数据采用了节省空间的类似词袋表征方式，以减少网络需要学习的参数个数。在文献[5]中作者用了CNN学习得到的非监督式“region embedding”来扩展模型，预测文字区域的上下文内容。这些论文中提到的方法对处理长文本（比如影评）非常有效，但对短文本（比如推特）的效果还不清楚。凭我的直觉，对短文本使用预训练的词向量应该能比长文本取得更好的效果。 搭建一个CNN模型结构需要选择许多个超参，上文中已经提到一些：输入的特征（word2vec, GloVe, one-hot），卷积滤波器的数量和尺寸，池化策略选择（最大值、平均值）以及激活函数（ReLU, tanh）。文献[7]通过多次重复实验，比较了不同超参在性能和稳定性方面对CNNs模型结构的影响。如果你想自己实现一个CNN文本分类模型，借鉴该论文的结果会是一个挺好的选择。其实验结论主要有：1）最大池化效果总是优于平均池化；2）选择理想的滤波器尺寸很关键，但也视情况而定；3）正则化处理在NLP任务中的效果并不明显。对于该结论需要注意的是，该研究所用文本集里的文本长度都相近，若是要处理长度不同的文本集，上述结论可能不具有借鉴意义。 文献[8]探索了CNNs在关系挖掘（Relation Extraction）和关系分类（Relation Classification）任务中的应用。除了词向量特征，作者还把词与词的相对位置信息作为卷积层的输入。这个模型假设了所有文本元素的位置已知，每个输入样本只包含一种关系。文献[9]和文献[10]使用的模型类似。 微软研究院的研究成果：文献[11]和 [12]介绍了CNNs在NLP的另一种有趣的应用。这两篇论文介绍了如何学习将句子表示成包含语义的结构，它能被用来做信息检索。论文中给出的例子是基于用户当前的阅读内容，为其推荐其它感兴趣的文档。句子的表征是基于搜索引擎的日志数据训练得到的。 大多数CNN模型以这样或是那样的训练方式来学习单词和句子的词向量表征，它是训练过程的一部分。并不是所有文献都关注这一步训练过程，也并不在乎学到的表征意义有多大。文献[13]介绍了用CNN模型对Facebook的日志打标签。这些学到的词向量随后又被成功应用于另一个任务 —— 基于点击日志给用户推荐感兴趣的文章。 字符层级的CNNs模型到目前为止，上面讨论的模型表征还都停留在单词层级。但是如何将CNNs模型直接用于字符也有一些相应研究。文献[14]通过学习字符层级的向量表征，结合预训练的词向量，以此达到给语音打标签的目的。文献[15]和[16]不依赖预训练词向量的这一步，研究用CNNs模型直接从字符学习。值得注意的是，这些研究作者使用了相对较深层的网络结构，达到了9层，以此完成语义分析和文本分类任务。结果显示，用字符层级的输入在大规模数据集（百万级）上学习的效果非常好，但在简单模型和小数据集（十万级）上的学习效果却是一般。文献[17]是关于字符级卷积运算在语言建模方面的应用，将字符级CNN模型的输出作为LSTM模型每一步的输入。同一个模型用于不同的语言。 令人惊讶的是，上面所有论文几乎都是发表于近一到两年。显然CNNs模型在NLP领域已经有了出色的表现，新成果和顶级系统还在层出不穷地出现。 参考文献[1]&nbsp;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751.[2]&nbsp;Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665.[3]&nbsp;Santos, C. N. dos, &amp; Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78).[4]&nbsp;Johnson, R., &amp; Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011).[5]&nbsp;Johnson, R., &amp; Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding.[6]&nbsp;Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., &amp; Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357.[7]&nbsp;Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification,[8] Nguyen, T. H., &amp; Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48.[9]&nbsp;Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., &amp; Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339.[10]&nbsp;Zeng, D., Liu, K., Lai, S., Zhou, G., &amp; Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344.&nbsp;[11]&nbsp;Gao, J., Pantel, P., Gamon, M., He, X., &amp; Deng, L. (2014). Modeling Interestingness with Deep Neural Networks.[12] Shen, Y., He, X., Gao, J., Deng, L., &amp; Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110.&nbsp;[13] Weston, J., &amp; Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827.[14]&nbsp;Santos, C., &amp; Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826.&nbsp;[15]&nbsp;Zhang, X., Zhao, J., &amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9.[16]&nbsp;Zhang, X., &amp; LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102.[17]&nbsp;Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2015). Character-Aware Neural Language Models.","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/tags/自然语言处理/"},{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/翻译/"}]},{"title":"深入理解卷积","date":"2017-09-22T12:28:55.000Z","path":"2017/09/22/Understanding_Convolutions/","text":"博客原文 [http://colah.github.io/posts/2014-07-Understanding-Convolutions/) 在前一篇博文中，我介绍了如何理解卷积神经网络（convolutional neural networks，CNN）,并没有涉及到太多的数学原理。如果想进一步了解，我们需要理解什么是卷积。 如果你仅仅是想理解CNN而已，那么粗略了解卷积就足够了。但是本系列的目的是带着大家更深入理解CNN并学习更多选择。因此，我们将需要十分深入的理解卷积。 幸运的事，通过几个例子，能够比较清楚的解释什么是卷积。 从丢掷小球例子谈起想象下我们从一个高处将一个球丢到地面，并且小球着地后只有一个运动方向。那么，将该小球丢掷一次后，并从它停滞的地方再次丢掷，通过这些操作，小球运动距离\\(c\\)的可能性是多少。 我们开始解决上述问题。第一次丢掷后，小球会以概率\\(f(a)\\)从开始点运动\\(a\\)个单位距离， 其中 \\(f\\) 是概率分布函数。 完成第一次丢掷后，我们拾起小球，在第一次停滞的地方从另一个高点再次丢掷，那么小球再次运动\\(b\\)个单位距离的概率是\\(g(b)\\)，如果两次丢掷的高度不同，那么\\(f\\)和\\(g\\)可能不一样。 为满足小球总的运动距离为\\(c\\)的假设，我们修复第一次丢掷的距离\\(a\\)，和第二次丢掷的距离\\(b\\)，那么有\\(a+b=c\\)，进而概率为 \\(g(a)\\cdot f(a)\\)。假设一个特殊例子，如果需要总的移动距离为3，第一次滚动距离为 \\(a=2\\)，为了满足 \\(a+b=3\\)，那么第二次滚动的距离为 \\( b=1\\)。 相应的概率为 \\( f(2)\\cdot g(1)\\)。 但是，这并不是达到总距离3的唯一方式。小球可以第一次滚动1，第二次滚动2.或者第一次滚动0，第二次滚动3，只要相加等于3，对于任何a和b都是满足条件的。 其概率分别为 \\(f(1)⋅g(2)\\) 和\\( f(0)⋅g(3)\\)。为获得小球滚动到\\(c\\)的总概率，我们不能仅考虑一种情况。相反，我们需要考虑到所有将\\(c\\)分割成\\(a\\)和\\(b\\)并且加和每种情况的概率。$$… f(0)⋅g(3) + f(1)⋅g(2) + f(2)⋅g(1) …$$我们已经知道对于 \\(a+b=c\\) 的概率是 \\(f(a)⋅g(b)f(a)⋅g(b)\\)。 因此，可以加和 \\(a+b=c \\)的所有解，可得全部的似然概率:\\[\\sum_{a+b=c} f(a) \\cdot g(b)\\]现在比较明了，我们正在做一个卷积操作。一般地， \\(f\\) 和 \\(g\\) 的卷积，在\\(c\\)处的值定义为：\\[(f\\ast g)(c) = \\sum_{a+b=c} f(a) \\cdot g(b)\\]用 \\(b = c-a\\) 替代可得：\\[(f\\ast g)(c) = \\sum_a f(a) \\cdot g(c-a)\\]这是卷积的标准定义。保证结果更可信，我们考虑小球可能滚动的距离。第一次丢掷后，小球以概率 (f(a))滚动到中间的位置 \\(a\\) 。如果小球停滞在 \\(a\\)，那么它会以概率 \\(g(c-a)\\)在第二次丢掷后滚动到 \\(c\\). 为得到卷积，我们考虑所有这样的中间位置。 卷积的可视化有一个很好的技巧帮助你更容易地理解卷积。 首先，一个比较明显的结论。 假设一个小球从它掉落的地方滚动到一个特定的距离 \\(x\\) 的概率是 \\(f(x)\\)。那么，他从停止的地方滚动 (x)的概率是\\(f(-x)\\)。 如果我们已知小球第二次丢掷后停止在 \\(c\\) ，那么第一次停止 \\(a\\)的概率是多少呢? 明显，前一次滚动距离 \\(a\\) 的概率是 \\(g(-(a-c)) = g(c-a)\\)。现在考虑每一个中间距离，使得最终的停滞距离为 \\(c\\)。我们知道第一次停滞的概率为\\(f(a)\\)。 我们也知道了如果第一次停留在 \\(a\\), 经过第二次停留在 (c) 的概率 \\(g(c-a)\\)。 对所有这样的 \\(a\\) 求和，我们便得到了卷积。这一步的好处是，使得在计算 \\(c\\) 处的卷积，用一张图谱就能够可视化了。 By shifting the bottom half around, we can evaluate the convolution at other values of (c)。这帮助我们较全面的理解卷积。For example, we can see that it peaks when the distributions align. And shrinks as the intersection between the distributions gets smaller. By using this trick in an animation, it really becomes possible to visually understand convolutions.Below, we’re able to visualize the convolution of two box functions: Armed with this perspective, a lot of things become more intuitive. Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy. 更高维的卷积卷积是一个一般的思想，我们可以将其用在更高维度的操作中。 我们再次以丢掷小球的为例。现在，当小球下落后，它滚动的方向并不是一个，而是两个。 和之前一样，卷积是:\\[(f\\ast g)(c) = \\sum_{a+b=c} f(a) \\cdot g(b)\\]不同的是。 现在 \\(a\\), \\(b\\) 和 \\(c\\) 都是向量。更明确的：\\[(f\\ast g)(c_1, c_2) = \\sum_{\\begin{array}{c}a_1+b_1=c_1\\\\ a_2+b_2=c_2\\end{array}} f(a_1,a_2) \\cdot g(b_1,b_2)\\]或者根据标准定义：\\[(f\\ast g)(c_1, c_2) = \\sum_{a_1, a_2} f(a_1, a_2) \\cdot g(c_1-a_1, c_2-a_2)\\]类似于一维度=卷积，我们可以将一个二维卷积看成是一个函数作用于另外一个函数的滑动操作，想乘或者相加.一个一般的应用就是图片处理。我们可以将图片看成是一个二维函数。许多重要的图片转换都是卷积，当对图像函数进行卷积操作处理是，用一个小的，局部函数，称之为“核函数”。 核函数滑过图片的每一部分，并将覆盖的像素值求和计算出一个新的像素值。例如，通过平均 3x3 像素，我们可以对图像进行模糊处理。为了达到这个目的，核可以对box的像素值都是 \\(1/9\\) ， 也可以通过采用相邻的像素值 \\(-1\\) 和 \\(1\\) ，其他都是0的核来锐化图片。也即是，通过相邻像素值相减。当相邻的像素值较相近时，得到的结果将会趋近于0。在边缘，像素值则会差别很大。 卷积神经网络（Convolutional Neural Networks）那么，卷积和卷积神经网络有什么联系呢？ 可以认为一个一维的卷积层，输入 \\({x_n}\\) 及输出 \\({y_n}\\),，像前一篇博客讨论的: 正如我们观察的，可以将输出通过输入表示： \\(y_n = A(x_{n}, x_{n+1}, …)\\)一般地，\\(A\\) 可以是多个神经元。但是目前仅仅认为是一个单一神经元。 回忆到一个典型的神经网络的神经元表述如下：\\[\\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)\\]其中 \\(x_0\\), \\(x_1\\)… 是输入。权值, \\(w_0\\), \\(w_1\\), … 表征了神经元怎么和输入联系起来的。负的权值表示输入值是抑制神经元激活，正的却是刺激神经元激活。权值是神经元的关键部分，控制了神经元的如何表现。 Saying that multiple neurons are identical is the same thing as saying that the weights are the same. It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us. 一般地，我们将神经元和层一起描述，而不是分开来说。这个关键点是权重矩阵，\\(W\\):\\[y = \\sigma(Wx + b)\\]例如，可得：\\[y_0 = \\sigma(x_1 + x_2 … )\\]\\[y_0 = \\sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)\\]\\[y_1 = \\sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)\\]每一行都描述了神经元连接到输入的权重。 再回到卷积层，然后，因为有许多相同神经元的肤质，许多权重出现在多个位置。 等效于如下方程： \\[y_0 = \\sigma(W_0x_0 + W_1x_1 -b)\\] \\[y_1 = \\sigma(W_0x_1 + W_1x_2 -b)\\]到此，一般地，权重举重使每个神经元连接每个输入通过不同的权重： \\[W = \\left[\\begin{array}{ccccc} W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\\\W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\\\ W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\\\ W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\\\ … &amp; … &amp; … &amp; … &amp; …\\ \\end{array}\\right]\\] 和上述相似的卷积层的矩阵看起来不尽相同。相同的权重出现在一连串的位置。并且由于神经元不会连接到许多的输入，因此这里有很多0. \\[W = \\left[\\begin{array}{ccccc} w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\\\ 0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\\\ 0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\\\ 0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\\\ … &amp; … &amp; … &amp; … &amp; …\\\\ \\end{array}\\right]\\] 将上述矩阵相乘和卷积操作 \\([…0, w_1, w_0, 0…]\\)一样。 函数滑过不同的位置，因为有不同的神经元在这些位置。 对于2-维卷积层什么呢？ The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution. 考虑用卷积来瑞华图片的例子，通过滑动核并且对每块都作用。和这个类似，卷积层会将神经元作用到每一部分的图像。 结论我们介绍了许多数学东西，也许他并不是很明显。卷积是一个有用的工具在概率理论和计算机图像，但是我们能够从处理卷积神经网络的到什么呢？ 第一个优势是，我们有一些很有用的语言来描述神经网络。上述例子还不是很复杂的来说明了这个问题，但是卷积让我们能够 but convolutions will allow us to get rid of huge amounts of unpleasant book-keeping for us. Secondly, convolutions come with significant implementational advantages. Many libraries provide highly efficient convolution routines. Further, while convolution naively appears to be an \\(O(n^2)\\) operation, using some rather deep mathematical insights, it is possible to create a \\(O(n\\log(n))\\) implementation. We will discuss this in much greater detail in a future post. 事实上，在GPUs上利用高性能分布式卷积计算已经在近些年的计算机视觉得到应用。 本系列的后续博文本博文是卷积神经网络系列的一部分何其一般化。前两篇博文会回顾熟悉深度学习，然后比较感兴趣对每一部分。如果想获得更新提醒，可以获取的 RSS ！ 致谢在此特别感谢 Eliana Lorch，关于对卷积的讨论和对本博文的支持。 同时也十分感谢 Michael Nielsen 和 Dario Amodei 的评论和支持。 1.我们想小球第一次滚动 \\(a\\) 第二次滚动\\(b\\) 。其概率分布为\\(P(A) = f(a)) 和 (P(b) = g(b)\\) 是独立的，并且分布在0对称。因此可得 \\(P(a,b) = P(a) * P(b) = f(a) \\cdot g(b)\\). 2.关于非标准的定义部分，我之前没有看到过，收益颇多。在以后的博文中，我们会发现这个定义会十分有用，因为它引领了数学机构。但是它也优势因为他使得许多卷积的数学特性十分明显。 例如卷积是一个累加操作。也就是说，\\(f\\ast g = g\\ast f\\)。 但是为什么呢？\\[(f\\ast g)(c) = \\sum_{a+b=c} f(a) \\cdot g(b)\\] \\[ \\sum_{a+b=c} f(a) \\cdot g(b) \\sum_{b+a=c} g(b) \\cdot f(a) \\] \\[ \\sum_{a+b=c} f(a) \\cdot g(b) = \\sum_{b+a=c} g(b) \\cdot f(a) \\]卷积同样是可交换的. That is,\\((f\\ast g)\\ast h = f\\ast (g\\ast h)\\)。 为什么呢? \\[\\sum_{(a+b)+c=d} (f(a) \\cdot g(b)) \\cdot h(c) = \\sum_{a+(b+c)=d} f(a) \\cdot (g(b)\\cdot h(c))\\]3.还有一些偏差，因为 “threshold” 神经元是否激活。但是这是十分简单的，并且我不想在本节来谈论。","tags":[{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://yoursite.com/tags/卷积神经网络/"},{"name":"卷积","slug":"卷积","permalink":"http://yoursite.com/tags/卷积/"}]},{"title":"深入理解长短期记忆模型","date":"2017-09-22T12:28:55.000Z","path":"2017/09/22/Understanding_LSTM_Networks/","text":"博客原文 [http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 循环神经网络简介（RNN）人们通常并不是每次都是从头开始思考问题，就如你在阅读本文时，你所理解的词语的含义是建立在前面的词语基础上的。你并不是把所有的历史信息都丢弃而从头开始理解，换言之，你的的思想是连续的。 传统的神经网络并没有克服这一点，这也看起来是它的巨大劣势。 例如，当你想区分在一部电影的每一个时刻发生了什么，如果用传统的神经网络，将无法利用已经发生的先验信息来理解后续的画面。 循环神经网络（Recurrent neural networks，RNN）克服了这些弊端。它们具有神经元循环作用，使得历史信息能够得以保留、延续。 循环神经网络结构 在上面的网络结构中，神经网络的一部分$A$，能够在$t$时刻输入 \\(x_t\\)，得到输出值 \\(h_t\\)。循环结构能够让从上一时刻的输出值作为下一次时刻的输入值。 这些循环结构让循环神经网络看起来有些神秘。然而，当你在思考下会发现，RNN和一般的神经网络并不是完全不同。一个循环神经网络可以看成是同一个网络的多个复制，每个网络都携带信息传入到下一个继承者。我们将RNN进行展开，可以得到： 一个展开的RNN 这个链状的神经网络显示了，RNN 和序列紧密联系。这种特性就是用来处理序列数据的。 当然，它也确实被用来处理这类数据。在过去的几年，RNNs在许多问题上都取得了不可思议的成果：语音识别，语言模型，翻译，图像标签等。我将不再过多讨论RNNs取得成功案例，更多可以参看 Andrej Karpathy’s 的优秀的博客The Unreasonable Effectiveness of Recurrent Neural Networks RNNs能够取得这些成功的主要因素还是 “LSTMs”的广泛使用，作为一种特殊的RNN，LSTM在很多任务上都比一般的RNN表现出色。几乎所有的在RNNs上的结构，都能够用LSTM很好的完成，这些LSTM就是本文要讨论的。 长期依赖的问题RNNs的一个启发思想是它能够将先前的信息保留用于当前的任务中，例如，利用先前的图像能够帮助理解当前画面。 如果RNNs能够达到这个要求，那么将会大有所用。事实上它也确实做到了。 有时候，我们仅需要关注最近的信息就足够完成当前的任务。例如，考虑用前面的单词来预测下一个单词的语言模型。如果我们想预测“the clouds are in the sky,” 的最后一个单词，我们不需要其他前面的上下文信息。很明显，句子的下一个单词就是sky。在这个例子中，相关的信息和需要的信息差异比较小，RNNs模型能够学习利用前文的信息。 但还是有很多情况，我们需要更多的上下文。设想你想预测 “I grew up in France… I speak fluent French.” 的下一个单词。临近的的信息表明，下一个单词是一个语言名称，但是如果我们想缩小语言范围，我们需要 France的上下文。极大可能的是，我们需要的信息和相关的信息差异很大。 不幸的事，当这个差异越来越大时，RNNs对链接这些信息开始变得无力。 神经网络在长信息依赖情况表现的很挣扎。 理论上，RNNs模型是绝对能够处理这类 “长期依赖”的问题。可以仔细选择参数来解决这样的问题。 可惜的是，在实践中，RNNs看起来并不总是能够学习到这些信息。这些问题在 Hochreiter (1991) [German] and Bengio, et al. (1994) 有更多的讨论， 并且提到了一些可能是导致这些困难的根本原因。 幸运的是，LSTMs模型能够处理这些问题！ LSTM神经网络长短时间记忆模型 – 通常被叫做 “LSTMs” – 是一种特殊的RNN，能够学习到长期依赖。由 Hochreiter &amp; Schmidhuber (1997)提出，并且在以下文献得到优化和推广。这些模型在很多情况下表现的出乎意料的好 ，如今也被广泛应用。 LSTMs 被主要设计是用来解决长期依赖的问题。记忆长期的信息是基本的模型特性，而不是很困难的才能学习到这些信息。 最近所有的神经网络都有很多复制网络的链式形式。 在标准的 RNNs，这种复制都有比较简单的结构，例如单个 tanh 层。 LSTMs 也有类似链状的结构，但是复制的模块有不同的结构。不是一个单一的神经网络层，他有四个，并且用特别的方式来交互。 LSTM中复制的模块包含四个交互的层。 不用太在意细节。我们会一步一步的来拆解LSTM。 现在，我们先熟悉后续需要用到的符号。 在上图，实线代表向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐点的运算，如向量加法。黄色框是学习神经网络层。实线合并表示串联合并。实线分岔表示其内容正在复制，副本将转到不同的位置。 LSTMs背后的核心思想LSTMs的关键是cell state，水平线穿过图标的顶部。cell state 就像一个传送带，但是他直接穿过整个链条，只有一些小的线性相互作用。这让信息可以流畅地传递下去且保持不变。 LSTM能够移除或者相加信息到cell state，通过阀门来有效的控制。 阀门是一个让信息穿过的选择方式。由 sigmoid 网络层和点乘运算组成。 sigmoid 层生成0到1之间的数字，体现了有多大的可能性让能通过。0意味着 “全不通过” 1意味着 “全都通过”。 一个LSTM存在三种这样的阀门，为了保护和控制cell state。 一步一步理解LSTMLSTM的第一步是决定了哪些信息能够从cell state丢弃。这些决定是被 sigmoid 层 被称作“遗忘门层” 。它接受\\(h_{t-1}\\) and \\(x_t\\),，并且在 cell state \\(C_{t-1}\\)输出一个 \\(0\\) 到 \\(1\\) 的数。\\(1\\) 表示 “全保留”当\\(0\\) 表示 “全丢弃”。 再次回到之前用上文来预测单词的语言模型。在这个例子中，cell state可能包括当前受试者的性别，从而可以使用正确的代词。当我们看到一个新主题时，我们想要忘记这个老主题的性别。 下一步是决定存储哪些新的信息在cell state。 这有两部分， 第一步， sigmoid 层被称之为 “输入门层” 决定了哪些值会更新。第二， tanh 层生成了一个新的候选值向量 \\(\\tilde{C}_t\\)， 这将会被加到state。下一步，我们将会整合这两部分的内容更新到state. 在我们的语言模型中，我们将新对象的性别添加到cell state中，以替换我们将要忘记的旧对象。 现在可以更新旧的 cell state \\(C_{t-1}\\)到新的 cell state \\(C_t\\)。之前的步骤已经决定了哪些会做，我只是完成而已。 通过 \\(f_t\\)乘以旧的 state,，来忘记之前决定要忘记的信息。然后会加上 \\(i_t*\\tilde{C}_t\\)。这是一个新的候选值，被我们想多大程度的进行放缩来更新值。 在语言模型的情况下，这个步骤就是用来放弃关于旧主题性别的信息，并添加新信息。 最后，我们需要决定我们需要输出什么。输出的信息是依赖 cell state而定的，但是一个过滤掉的版本。第一，我们会通过 sigmoid 层决定那部分的 cell state 会作为输出。然后，将 cell state 通过 \\(\\tanh\\) (to push the values to be between \\(-1\\) 和 \\(1\\)) 和 相乘 通过输出的 sigmoid 门，以便得到我们想要的输出信息。 例如语言模型例子，当看到一个对象，他可能希望我们输出一个关于动词的信息，这就是下一个实现的。例如，它可能会输出对象是单数还是复数，以便我们知道动词应该如何组合在一起，如果这是接下来的内容。 LSTM的一些变体前面讨论的都是一个很普通的LSTM。并不是所有的 LSTMs 都和上面一样。事实上，几乎所有的论文在涉及到LSTM都会提到不同的版本。这些差异都很小，但有些值得关注。 一个比较流行的LSTM辩题，由Gers &amp; Schmidhuber (2000) 提出，就是添加 “peephole connections.” 。意味着他让我们的 gate layers 接受来自cell state的数据。 上面的图形增加 peepholes到所有的门，但是许多文章只使用了部分 peepholes。 另外一变体，就是利用耦合遗忘和输入门。而不是单独的决定哪些遗忘和哪些更新信息，将这些决定综合起来。只当我们要在输入内容时忘记。当我们忘记旧的东西时，我们只会向cell state中输入新的值。 一个有趣的 LSTM 变体是 Gated Recurrent Unit，或者叫 GRU，由Cho, et al. (2014) 提出。他这个遗忘和输入门成单一的 “更新门” 。同时合并了 cell state 和 hidden state，以及其他的一些改变。所得的模型比一般的LSTM还要简单，现在已经变得越来越流行。 上面只是少数值得关注的 LSTM的变体。还有其他的，例如 Depth Gated RNNs by Yao, et al. (2015)。还有一些完全不同的处理长期依赖的方法，例如 Clockwork RNNs by Koutnik, et al. (2014)。 但是哪些变体是最好的呢？或者这些差异明显吗？ Greff, et al. (2015) 做了一个比较，发现他们都差不多。 Jozefowicz, et al. (2015) 测试了超过万次的 RNN architectures，发现又一些在一个特殊的任务下是优于LSTMs的。 结论前面，我提到了一些RNNs取得的一些非凡的进展。几乎所有的这些都能够利用 LSTMs 得到一样的结果。在有些任务上表现的更好！ 如果写下一组公式， LSTM 看起来十分不友好。通过本文一步一步理解，能够使得LSTM更容易接受。 LSTMs 是优化RNNs成功的一大步。理所当然会想到：是否还有其他方式呢？在研究者的共同观点是“是的，有下一步，且值得关注！” 。主要思想是RNN的每一步都提取跟多的信息。例如，你想用RNN来给图像来创建一个标签，可能需要提取图片的每一部分来寻找输出的所有单词。事实上， Xu, et al. (2015) 却是这样做的 – 可能是一个有趣的起点！有一些真正令人兴奋的结果使用注意力，似乎还有更多的是在角落… 注意力不是RNN研究中唯一令人兴奋的线索。例如， 由 Kalchbrenner, et al. (2015) 提出的Grid LSTMs，似乎很令人振奋。在 generative 模型中也有用到RNNs –例如 Gregor, et al. (2015), Chung, et al. (2015), 和 Bayer &amp; Osendorfer (2015) – 也很有趣。 最近几年对于卷积神经网络来说已经是一个激动人心的时刻了，下一个希望只有更加如此！ 致谢感谢许多朋友帮助我更好的理解 LSTMs，评论可视化，和关于本博客提供反馈。 非常感谢我的google的同事的帮助，尤其是 Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever。也十分感谢我的朋友和同事抽出时间帮助我。 包括 Dario Amodei, and Jacob Steinhardt。特别感谢 Kyunghyun Cho for extremely 来对图形的支持。 在博文之前，我解释了 LSTMs 在两个 seminar series 我分享 neural networks。感谢所有协同的人。 还有原作者，需要对LSTM模型做出贡献的人。一个不完全的名单列表：Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves。","tags":[{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/神经网络/"}]}]